<meta charset="utf-8" emacsmode="-*- markdown -*-"><link rel="stylesheet" href="../style.css?">

**How much boilerplate code you need to write a ray-tracer?**

# Introduction

If you want to write a ray-tracer from scratch you would usually start from something simple, like **Ray Tracing in One Weekend**. 
There will be spheres, planes, rays. Eventually you want to render a bunny, so you will add simple 3D model loading, which would require 
adding triangles, and routines for triangle intersection, then area light sources, envrionment images, participating media... 

The list could be continued forever. And then at some point you realize that you want to render a caustics (either from refractive from glass,
or reflective from metals). And now you need to learn stuff about bidirectional renders, and add even more code to make it work.

In this post I want to give an idea how much code is needed to write a bidirectional ray-tracer, which can produce images like this:

![](images/beams.png width="192px")![](images/volumetric-refractions.png width="192px")

The purpose of this post is to rather serve as a checklist and starting point, than scary and frustrate people who are learning ray-tracing. 
Also it describes my personal experience.

So let's take a closer look!

# Code for unidirectional ray-tracer

Let's pretend we are writing a unidirectional path-tracing from scratch. 

## Images
The first thing you would want to have is to see a result of your work. Therefore you need an ability to save an image. 

Right now we are not talking about UI and visualizing images on the screen, which is rather important feature 
together with debug visualization. Let's just start from saving a result to a file. A lot of projects using something like PPM file format, which 
is very simple, but in the end you need to write an extra code for this simplicitly (like tone mapping, converting from floating point values to uint8, etc)

That's why I propose to have an output in `float4` (RGBA32F) format and use **[LINK]** tinyexr library for writing HDR images in EXR format. 
This library would also be useful in the future for loading EXR images.

## Intersections
Now you want to actually trace something. There is at least three ways to it: 
- write all intersection and BVH code by yourself;
- build and use open-source third-party library;
- use something like **[LINK]** Intel Embree;

I'd personally recommend going with the third option. Direct comparison between Embree and couple of other libraries from GitHub shows that 
with Embree ray-tracer could be easily 2x faster even when using the most naive way (tracing one ray without batching them). 
And the integration and usage of Embree is quite simple.

But of course it highly depends on your personal motivation and purposes. If you want to get an exeprience writing everything from the scratch - just do it!

Having the intersection routines you now can hardcode a couple of triangles and see if everything works. Let's move to the actual geometry!

## Scene 
Now you want to load some geometry from files. The easiest way to do it would be using something like **[LINK]** tinyobjloader. I think OBJ is still 
one of the most popular format around because it's simplicity. Of course you might want to load GLTF files, or even something more complicated. 
But the point is that now you have a bunch of triangles and vertices which you uploaded to your intersection library and you desperate to trace them.

At this point you can hardcode some camera parameters and throw some rays, and actually see a result. But it's good time to add a camera to your ray-tracer.

## Camera
The easiest way to setup a camera in the scene would be to define a position and a view point, and a field of view. 
If you are loading format which contains camera data (like GLTF) - it would be even simpler. 
But anyway you would need a code which builds camera data (like matrices). 

And now having a camera data you could easily generate rays for the specific camera configuration. Going through each pixel of the image you need to 
generate a ray for this specific pixel, so there should be two functions - to get NDC coordinates from a pixel, and to generate ray out of this coordinates.

At this point you'd probably could see something like that in output:
![](images/barycentrics.png width="192px")![](images/normals.png width="192px")

And this is definitely a good start, but let's move on.

## Multithreading
So far we were using a simple loop over all pixels in image. But this is not the fastest way. We usually want to spread a workload over all threads we 
have and get a result as fast as possible. Any option is suitable - you could write your own scheduler or use a library. I am using 
[enkiTS](https://github.com/dougbinks/enkiTS) with a wrapper around.

## Materials
At this point you could write a fully functional ambient occlusion integrator which launches rays in the scene, and the secondary rays to see if geometry
is occluded. And when it will be working - is a good time to add a colors to your renderings. For that purpose you will be need to introduce materials.
Usually you could start from simple diffuse materials, and then eventually add more and more of them.

Also now we can enable a very simple light sources. Just use emissive color of material, and if ray hit such material - it will contribute to the ray's
result.

And since we are using materials - we can now start using importance sampling. The very first method would be to sample material's BSDF 
to determine the best next direction for ray, rather than selecting a direction randomly in hemisphere around normal. 
Each material has it's own "preferred" directions, but in general - the most commonly used ones are:
- cosine-weighted direction on a hemisphere (for diffuse materials);
- perfect specular reflections and refractions (basically `reflect` and `refract` functions) - for mirror and glass materials;
- some normal distribution (GGX is most commonly used now) for more complex reflections and refractions.

Also if we want to render realistic metals or glass materials - we should take care of Fresnel coefficients. 
They are calculated differently for metals and non-metals, which adds a bit more boilerplate code.

## Light sources
So far so good. We are sending rays, which bounces around on our scene and gathering light. And after waiting some time our algorithm would 
be able to produce image like this:

![128 samples per pixel](images/cb-step-1.png width="192px")

Looks good, but look on those noise. The whole image is noisy even using only diffuse materials. To fix this issue we have a Next Event Estimation,
or simply sending an additional rays directly to a light sources. At the first sight looks simple - we have a list of emissive triangles on a scene,
and we just randomly picking one and sending a "shadow" ray to it. If this ray reaches emissive triangle - we adding it's contribtion to the ray's radiance.
But! If on some scene we have two triangle, one of which is really huge and covers like half of a scene, and a second one is microscopic, and picking them
randomly (each with 50% probability) - then contribution from microscopic triangle will be neglectible, and image would still have a lot of noise.

Whan we can do in this situation? Pick lights not randomly, but based on their contribution to the whole lighting in a scene. And here comes an 
additional code, which would also will greatly serve us in a future. I am talking about generating and sampling distributions tables 
(see links section for the some great posts about generating them). So another piece of boilerplate code would be to generate and sample random numbers,
based on some random value distributions. In our example we would want to sample huge emissive triangle in 95% of cases, and small one only in 5%.

Now we have a helpers which allow us to sample proper light sources and we are ready for plug in Multiple Importance Sampling. Basic idea behind it is
to weight a light contribution according to a probabilities of sampling light source and sampling BSDF. And here we need another piece of boilerplate code
which randomly selects an emitter and evaluates probability of sampling it.

Now we can render the same image with same 128 samples per pixel, and get this result:

![128 spp](images/cb-step-1.png width="192px") ![128 spp + MIS](images/cb-step-2.png width="192px")

## Textures
Now to add another dimension to our rendering we would want to use textures. For that we need routines to load images and evaluate them at the certain
coordinates. For that I'd suggest to use libraries like [LINK] tinyexr and [LINK] stb_image. Also keep in mind that different materials might refer
to the same images, which adds another piece of boilerplate code - to manage images and save some memory.

## Environment images / distant emitters
So far we were using only emissive triangles to illuminate our scene.

## Medium

# Bidirectional path tracing

## Output image wrapper

## New methods for emitters and camera

# Spectral Rendering

## Basics

## Spectrums

## Refraction Indices

# Conclusion
As a recap we can make a list of methods/routines which are used in ray-tracer:
- writing output images;
- intersection routines;
- scene loading code;
- camera code:
  - generating rays (considering depth of field);
  - sampling image plane;
  - evaluating pdf and importance for a given ray;
- multithreading code (considering multithreaded writes to output image);
- materials and BSDF code:
  - sampling BSDF; 
  - evaluating BSDF for a given ray;
  - evaluating probabilities for a given ray;
  - **everything above times N materials**;
- light sources:
  - building a distribution table for light sources in scene;
  - sampling light sources accorindg to the distribution table;
  - evaluating light source contribution for a given ray;
  - sampling an emission from light source (position, direction, contribution, probabilities);
  - **everything above times M light source types**;
- medium rendering:
  - calculating transmittance;
  - sampling medium;
  - sampling phase functions;
  - code for loading .vdb files for heterogeneous medium;
- spectral rendering:
  - code for handling spectral distributions (creating, loading, evaluating);
  - converting spectral values to XYZ/RGB color spaces;
  - sampling wavelengths;
  - ability to switch between RGB or spectral rendering.

As you see the real amount of code if really huge. And we did not even touch GPU code.

But you should not be scared, because every other piece of code could be added on top of existing code to improve your renderings. 
Start from small steps and eventually you will be able to render a beautifull images!

# Support

# Links

[Generating Random Numbers From a Specific Distribution By Inverting the CDF](https://blog.demofox.org/2017/08/05/generating-random-numbers-from-a-specific-distribution-by-inverting-the-cdf/)

<style class="fallback">body{visibility:hidden}</style><script>markdeepOptions={tocStyle:'long'};</script>
<!-- Markdeep: --><script src="https://casual-effects.com/markdeep/latest/markdeep.min.js?" charset="utf-8"></script>
